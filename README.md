# Deploying Llama2 with NVIDIA Triton Server tutorial

In this repository, we will give an example on how to efficiently package and deploy Llama2, using NVIDIA Triton Inference Server to make it production-ready in no time.

We will cover three different approaches:
- [Using HuggingFace models with Triton’s Python Backend](./python_backend/README.md)
- [Using HuggingFace models with Triton’s Ensemble models](./ensemble_model/README.md)
- [Using the vLLM framework](./vLLM/README.md)


For further reading visit our [blog](https://blog.marvik.ai/2023/10/05/deploying-llama2-with-nvidia-triton-inference-server/).

For a feature by feature explanation, refer to the Triton Inference Server [documentation](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html).
